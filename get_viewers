import requests
import datetime
import time
import os
import csv
from typing import List
from dotenv import load_dotenv
load_dotenv()  # load environment variables from .env file

class twitchchatlogger:
    def __init__(self, client_id: str, client_secret: str):
        self.client_id = client_id
        self.client_secret = client_secret
        self.access_token = self.get_access_token()
        self.headers = {
            'Client-ID': self.client_id,
            'Authorization': f'Bearer {self.access_token}'
        }
        self.data = []

    def get_access_token(self) -> str: #Get an OAuth token using client ID and secret.
        url = 'https://id.twitch.tv/oauth2/token'
        params = {
            'client_id': self.client_id,
            'client_secret': self.client_secret,
            'grant_type': 'client_credentials'
        }
        response = requests.post(url, params=params)
        response.raise_for_status()
        return response.json()['access_token']

    def get_top_channels(self, limit=1000) -> List[tuple]:
        url = 'https://api.twitch.tv/helix/streams'
        channels = []
        cursor = None
        total_fetched = 0

        while total_fetched < limit:
            params = {'first': 100}
            if cursor:
                params['after'] = cursor

            response = requests.get(url, headers=self.headers, params=params)
            response.raise_for_status()
            data = response.json()

            for stream in data['data']:
                user_login = stream['user_login']
                viewer_count = stream['viewer_count']
                channels.append((user_login, viewer_count))

            total_fetched += len(data['data'])
            cursor = data.get('pagination', {}).get('cursor')
            if not cursor:
                break

            time.sleep(0.5)

        return channels[:limit]
    
    def get_channels_above_viewer_no(self, min_viewers=10000, max_pages=5000): #sets minimum viewer count
        url = 'https://api.twitch.tv/helix/streams'
        user_logins = []
        cursor = None
        pages = 0

        while pages < max_pages:
            params = {'first': 100}
            if cursor:
                params['after'] = cursor

            response = requests.get(url, headers=self.headers, params=params)
            response.raise_for_status()
            data = response.json()

            batch = data.get('data', [])
            if not batch:
                break

            for stream in batch:
                viewer_count = stream['viewer_count']
                if viewer_count <= min_viewers:
                    print(f"Stopped at {stream['user_login']} with {viewer_count} viewers")
                    return user_logins
                user_logins.append(stream['user_login'])

            pages += 1
            cursor = data.get('pagination', {}).get('cursor')
            if not cursor:
                break

            time.sleep(0.4)  # Be kind to Twitch

        return user_logins
    
    def fetch_chatters(self, channel): #get chatters using the TMI endpoint.
        url = f"https://tmi.twitch.tv/group/user/{channel.lower()}/chatters"
        print(f"Fetching chatters for: {channel} -> {url}")  # Debug print
        try:
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                data = response.json()
                chatters = []
                for role, users in data["chatters"].items():
                    chatters.extend(users)
                return chatters
            else:
                print(f"TMI returned status {response.status_code} for {channel}")
                return []
        except requests.RequestException as e:
            print(f"RequestException for {channel}: {e}")
            return []

    def collect_data(self, channels_with_viewers: List[tuple]):
        self.data.clear()
        timestamp = datetime.datetime.now().isoformat(timespec='seconds')

        for i, (channel, total_viewers) in enumerate(channels_with_viewers, start=1):
            chatters = self.fetch_chatters(channel)
            chatters_count = len(chatters)

            self.data.append({
                "timestamp": timestamp,
                "channel": channel,
                "total_viewers": total_viewers,   # from Helix API
                "chatters_count": chatters_count, # from TMI
                "chatters": chatters})

            print(f"[{i}/{len(channels_with_viewers)}] {channel} - {total_viewers} viewers, {chatters_count} in chat")
            time.sleep(0.25)

    def save_to_csv(self, filename="chatters_log.csv"):
        directory = os.path.dirname(filename)
        if directory:
            os.makedirs(directory, exist_ok=True)

        with open(filename, mode='w', newline='', encoding='utf-8') as file:
            writer = csv.writer(file)
            writer.writerow([
                "timestamp", "channel", "total_viewers", "chatters_count", "chatters"
            ])

            for entry in self.data:
                writer.writerow([
                    entry["timestamp"],
                    entry["channel"],
                    entry["total_viewers"],
                    entry["chatters_count"],
                    ', '.join(entry["chatters"])])

    def print_report(self):
        for entry in self.data:
            print(f"\nChannel: {entry['channel']}")
            print(f"Time: {entry['timestamp']}")
            print(f"Viewers: {entry['total_viewers']}")
            for user in sorted(entry['chatters']):
                print(f"  - {user}")

if __name__ == "__main__":
    CLIENT_ID = os.getenv("TWITCH_CLIENT_ID") or "your_client_id"
    CLIENT_SECRET = os.getenv("TWITCH_CLIENT_SECRET") or "your_client_secret"

    logger = twitchchatlogger(CLIENT_ID, CLIENT_SECRET)

    top_channels = logger.get_top_channels(limit=10)
    logger.collect_data(top_channels)
    logger.print_report()
    logger.save_to_csv("chatters_log.csv")
    print("Saved chat data to chatters_log.csv")