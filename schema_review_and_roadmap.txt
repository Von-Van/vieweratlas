================================================================================
  VIEWERATLAS — SCHEMA REVIEW & ROADMAP
  Generated: 2025-02-05  |  Last updated: 2026-02-05
================================================================================

This document reviews "vieweratlas scheme.txt" against the actual codebase,
catalogs errors, inconsistencies, and drift, then maps out remaining work.
Note: "vieweratlas scheme.txt" is currently outdated; treat this document as
the source of truth until the schema file is refreshed.


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
PART 1 — ERRORS & ISSUES IN THE SCHEMA DOCUMENT
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

  Issues 1–3 and 5–8 have been RESOLVED — the schema document was updated
  to match the codebase (TMI struck, TwitchIO documented, sources filled in,
  VOD pipeline and S3 storage sections added, config presets documented).

  The following observations remain relevant for future work:

4. SCALE UNDERESTIMATE FOR PAIRWISE COMPARISON
   ─────────────────────────────────────────────
   The schema states the O(n²) pairwise comparison "is manageable" because
   "we often limit to top channels (e.g. 100 or a few hundred)." But the
   default config sets `top_channels_limit: 5000`, and the rigorous preset
   also targets 5000. At 5000 channels that's ~12.5 million pairs — still
   feasible in Python but takes meaningful time (minutes to tens of minutes
   depending on set sizes). The schema's phrasing underplays this.

7. REPEAT-INTERACTION WEIGHTING IS ASPIRATIONAL ONLY
   ──────────────────────────────────────────────────
   The schema devotes a paragraph to weighting edges by repeat viewer loyalty
   (e.g., "summing a weight for each shared user where the weight might be
   the minimum of that user's visit counts in A and B"). The codebase does
   not implement this — edges are strictly unique-user-intersection counts.
   The schema should mark this clearly as a future enhancement rather than
   presenting it as part of the current design.

  Scope clarifications (2026-02-05):
  - Collection is chatters-only via IRC (no Helix Get Chatters; no mod tokens)
  - Target scale is top 5000 channels checked at regular intervals
  - VOD check: if a VOD was created in the last 24 hours, process it once
  - Daily de-duplication: only one chatter list per channel per day for live
    and one per channel per day for VOD


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
PART 2 — BUGS IN THE CODEBASE (found during review)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

  ALL SIX BUGS (A–F) HAVE BEEN FIXED.

  A. ✅ Escaped quotes in get_viewers.py lines 114–115 → replaced with "
  B. ✅ Escaped quotes in main.py mode_preprocess_vods → buggy block removed
  C. ✅ Duplicate VOD processing loop in main.py → first (buggy) copy deleted
  D. ✅ Duplicate storage init in PipelineRunner.__init__ → second copy deleted
  E. ✅ Phantom game_threshold / language_threshold kwargs → removed from loader
  F. ✅ top_channels_limit read from wrong YAML section → now reads collection;
       collection_interval key mismatch → now reads collection_interval_minutes


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
PART 3 — SCHEMA ↔ CODEBASE DRIFT SUMMARY
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

  Schema Section               Codebase Reality              Status
  ─────────────────────────    ─────────────────────────     ────────
  TMI chatters endpoint        TwitchIO IRC only             ✅ Fixed
  python-twitch-client lib     twitchio + requests            ✅ Fixed
  In-memory dicts only         FileStorage + S3Storage        ✅ Fixed
  No VOD support               Full VOD pipeline              ✅ Fixed
  SQLite suggestion            File + S3 (not SQLite)         ✅ Fixed
  Single config file           Dataclass presets + YAML       ✅ Fixed
  Missing source citations     All 13 sources defined         ✅ Fixed
  Repeat-viewer weighting      Not implemented                Remaining
  Daily de-dup + VOD 24h check Not implemented                Remaining
  "100 or a few hundred"       Default target is 5000         Remaining
  Basic logging mention        RotatingFileHandler + CW       Remaining
  No deployment discussion     Docker + ECS + EventBridge     Remaining


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
PART 4 — ROADMAP
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━


COMPLETED ITEMS
───────────────

  ✅ Fix syntax errors in get_viewers.py and main.py
  ✅ Remove duplicate code in main.py
  ✅ Fix config.py load_config_from_yaml bugs
  ✅ Update the schema document to match implementation
  ✅ Add pytest test suite (53 tests across 6 test classes)


TIER 1 — DEPLOYMENT-READY (make it runnable on AWS)
────────────────────────────────────────────────────

  Existing infrastructure files in infrastructure/aws/ provide a solid
  starting point (ECS task definitions, IAM roles, EventBridge schedules,
  Athena schema, monitoring dashboard, deploy scripts). The items below
  close the gaps needed to actually go from "git clone" to "running on AWS."

  6. Normalize metadata keys across the pipeline
     DataAggregator outputs "game_name" / "viewer_count" but ClusterTagger
     reads "game" / "viewers" / "language". This mismatch means tagging
     produces generic labels ("Community N") instead of meaningful ones when
     run end-to-end. Fix:
     - Standardize on a single key set in _ingest_snapshot and ensure the
       language field is always propagated (from both live and VOD paths)
     - Update ClusterTagger or add a normalization step in the pipeline
     - Update tests to verify the full aggregator→tagger path works

  7. Validate and harden Docker images for deployment
     The three Dockerfiles exist but may not build cleanly after the code
     fixes. Tasks:
     - Verify all three images build successfully (collector, analysis, vod)
     - Run each container locally with docker-compose to confirm it starts
     - Ensure the collector container's CMD actually invokes the right
       entry point (get_viewers.py runs the IRC bot; main.py is for
       pipeline modes)
     - Pin the TwitchDownloaderCLI version in Dockerfile.vod instead of
       pulling "latest" (reproducible builds)

  8. Parameterize AWS infrastructure templates
     The ECS task definitions, IAM roles, and EventBridge schedules use
     ${AWS_ACCOUNT_ID}, ${AWS_REGION}, ${S3_BUCKET}, ${EFS_ID} placeholders
     that deploy.sh resolves via sed. Tasks:
     - Add a .env.example with all required variables documented
     - Validate that deploy.sh and safe-deploy.sh handle missing variables
       gracefully (safe-deploy.sh already checks S3_BUCKET; deploy.sh
       does not)
     - Add Secrets Manager setup steps (Twitch OAuth + Client ID) to the
       deployment documentation
     - Confirm the EFS volume mount in ecs-task-collector.json is needed
       or remove it (the collector writes to S3, not local disk)

  9. Create a step-by-step deployment guide
     There is no single document explaining how to go from zero to running.
     Create DEPLOYMENT.md covering:
     a) Prerequisites (AWS account, CLI configured, Docker, Twitch app creds)
     b) One-time setup:
        - Create S3 bucket and configure lifecycle rules
        - Store Twitch credentials in Secrets Manager
        - Create ECR repositories
        - Create ECS cluster (Fargate)
        - Create CloudWatch log groups
        - Create VPC / subnets / security group (or use defaults)
        - Set up SNS topic for alerts (reference existing SNS_SETUP.md)
     c) Build and deploy:
        - Build Docker images → push to ECR
        - Register ECS task definitions
        - Create ECS services (collector as long-running, analysis + VOD
          as scheduled tasks via EventBridge)
     d) Verify:
        - Check CloudWatch logs for collector output
        - Confirm S3 snapshots are landing in raw/snapshots/
        - Trigger a manual analysis run
     e) Cost controls:
        - Budget alerts (safe-deploy.sh already sets $50/month)
        - S3 lifecycle (raw logs 30 days, VOD raw 7 days, curated→Glacier 90)
        - CloudWatch log retention (7 days)
        - Task-level max_runtime_hours / max_collection_cycles in config.yaml

  10. Add environment variable override for all config fields
      The config system supports env var overrides but not all fields are
      wired up. For containerized deployment, every config field should be
      settable via environment variable so ECS task definitions can fully
      control behavior without mounting a config.yaml file. Convention:
        VIEWERATLAS_COLLECTION_BATCH_SIZE → collection.batch_size
        VIEWERATLAS_ANALYSIS_OVERLAP_THRESHOLD → analysis.overlap_threshold
        etc.

  11. Wire up CloudWatch custom metrics
      The monitoring-dashboard.yaml defines metrics (VODQueuePending,
      VODsProcessedSuccess, etc.) and the YAML includes sample Python code
      to emit them, but this code is not integrated into the actual
      vod_collector.py or main.py. Tasks:
      - Create a metrics.py module wrapping boto3 CloudWatch put_metric_data
      - Call it from VODCollector after each VOD is processed
      - Call it from PipelineRunner at each pipeline step
      - Make it optional (no-op when running locally without AWS creds)


TIER 2 — SHORT-TERM IMPROVEMENTS
─────────────────────────────────

  12. Enforce daily de-dup + last-24h VOD check
      Collector should run at regular intervals but only record one chatter
      list per channel per calendar day (UTC recommended) for live streams.
      If a VOD was created in the last 24 hours, download and parse its chat
      once, then skip further VOD collection for that channel that day.

  13. Improve edge weighting with repeat-viewer scoring
      The schema describes this in detail but it's unimplemented. A simple
      v1: for each shared user U between channels A and B, weight their
      contribution by min(snapshots_A(U), snapshots_B(U)). This requires
      tracking per-user-per-channel appearance counts in DataAggregator
      (currently it only stores sets, not counts).

  14. Add data retention and cleanup to the application layer
      safe-deploy.sh already configures S3 lifecycle rules, but the
      application itself has no pruning logic. Add:
      - Configurable retention window (e.g., 30 days) in PipelineConfig
      - Automatic pruning of old local snapshots in FileStorage
      - A "cleanup" mode in main.py that removes expired data

  15. Bot detection and filtering
      Chat data is noisy with bots (Nightbot, StreamElements, Moobot, etc.)
      and known bot accounts. Add:
      - A known-bots blocklist (many public lists exist)
      - Heuristic detection (accounts appearing in 50%+ of all channels)
      - Optional flag in config to enable/disable bot filtering


TIER 3 — MEDIUM-TERM ENHANCEMENTS
──────────────────────────────────

  16. CI/CD pipeline with automated AWS deployment
      No CI exists. Set up:
      - GitHub Actions for lint (ruff/flake8), type check (mypy/pyright),
        and test (pytest) on every PR
      - Docker image build + push to ECR on merge to main
      - Optional: automated ECS service update after ECR push
      - Scheduled nightly analysis run via EventBridge (config already
        exists in eventbridge-schedules.json)

  17. Web-based visualization dashboard
      The current output is a static PNG + an HTML file from PyVis. A proper
      dashboard would offer:
      - Zoomable, pannable graph (D3.js / Sigma.js / Cytoscape.js)
      - Community filter/highlight controls
      - Streamer search with fly-to
      - Time-series view showing community evolution over time
      - Click-through to Twitch channel pages
      Could be hosted on S3 + CloudFront as a static site built from
      analysis output, or a small FastAPI app on ECS.

  18. Temporal community tracking
      Currently each analysis run produces a snapshot partition. To track
      how communities evolve:
      - Store historical partition results with timestamps in S3
      - Compute community similarity (Jaccard, NMI) between successive runs
      - Detect community births, deaths, merges, and splits
      - Visualize as a Sankey / alluvial diagram

  19. Better Louvain alternatives
      The Louvain algorithm is a solid default, but Leiden (leidenalg) is
      strictly better — it's faster and avoids the "poorly connected
      communities" problem that Louvain can produce. The python-louvain
      package hasn't been updated since 2022. Consider:
      - Adding leidenalg as a dependency option
      - Making algorithm choice configurable (louvain / leiden / label_prop)
      - The CommunityDetector already has a SimpleGreedyCommunityDetector
        fallback; extend this pattern to support multiple backends

  20. Athena integration for ad-hoc analysis
      The Athena schema already exists (athena-schema.sql) with partition
      projection and example queries. To make it usable:
      - Automate Glue catalog / table creation as part of deploy
      - Add a "query" mode to main.py that runs canned Athena queries
        and dumps results to CSV / stdout
      - Use Athena for the aggregation step at scale (>5000 channels)
        instead of loading everything into memory


TIER 4 — LONGER-TERM VISION
────────────────────────────

  21. Multi-platform support (YouTube, Kick, etc.)
      The schema describes a BaseCollector interface. This is partially
      realized — the storage abstraction exists, but the collector itself
      is tightly coupled to Twitch. Steps:
      - Define a formal BaseCollector ABC
      - Refactor ChatLogger into TwitchCollector implementing that ABC
      - Add YouTubeCollector, KickCollector
      - The graph builder and downstream modules need no changes

  22. Cross-platform community detection
      Once multi-platform data exists, build a unified graph where nodes
      are creators (not platform-specific channels) and edges are shared
      viewers across platforms. Requires identity resolution (matching a
      Twitch user to their YouTube account).

  23. Overlapping community detection
      - Clique percolation (networkx has it) for small graphs
      - BigCLAM or DEMON for larger graphs
      - Visualization becomes harder (mixed-color nodes, Venn overlays)

  24. Social media enrichment
      Twitter followers, subreddit size, Discord, TikTok as supplementary
      signals to enrich node metadata and validate detected communities.

  25. Scalability to full Twitch
      At 5,000+ channels with millions of unique viewers:
      - Replace in-memory sets with DuckDB or Athena
      - Sparse matrix representations for overlap computation
      - Approximate set intersection (MinHash / HyperLogLog)
      - Distributed community detection (igraph / graph-tool)
      - The Athena schema suggests this direction is already planned


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
PART 5 — AWS DEPLOYMENT ARCHITECTURE (reference)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

  This section summarizes the target AWS architecture based on existing
  infrastructure files in infrastructure/aws/.

  ┌─────────────────────────────────────────────────────────────────────┐
  │                        AWS ARCHITECTURE                            │
  │                                                                    │
  │  ┌──────────────┐   ┌───────────────┐   ┌──────────────────────┐  │
  │  │ ECR          │   │ Secrets Mgr   │   │ EventBridge          │  │
  │  │ 3 repos:     │   │ twitch/       │   │ Schedules:           │  │
  │  │  -collector  │   │  oauth_token  │   │  -collector: always  │  │
  │  │  -analysis   │   │  client_id    │   │  -analysis: daily    │  │
  │  │  -vod        │   │               │   │  -vod: every 6h      │  │
  │  └──────┬───────┘   └──────┬────────┘   └──────┬───────────────┘  │
  │         │                  │                    │                  │
  │         ▼                  ▼                    ▼                  │
  │  ┌─────────────────────────────────────────────────────────────┐   │
  │  │                     ECS FARGATE CLUSTER                    │   │
  │  │                   "vieweratlas-cluster"                    │   │
  │  │                                                            │   │
  │  │  ┌─────────────┐  ┌──────────────┐  ┌──────────────────┐  │   │
  │  │  │ Collector   │  │ Analysis     │  │ VOD Collector    │  │   │
  │  │  │ 0.25 vCPU   │  │ 1 vCPU       │  │ 0.5 vCPU         │  │   │
  │  │  │ 512 MB      │  │ 2 GB         │  │ 1 GB             │  │   │
  │  │  │ Long-running│  │ Scheduled    │  │ Scheduled        │  │   │
  │  │  │ (IRC bot)   │  │ (on-demand)  │  │ (EventBridge)    │  │   │
  │  │  └──────┬──────┘  └──────┬───────┘  └──────┬───────────┘  │   │
  │  └─────────┼────────────────┼─────────────────┼──────────────┘   │
  │            │                │                  │                  │
  │            ▼                ▼                  ▼                  │
  │  ┌─────────────────────────────────────────────────────────────┐   │
  │  │                      S3 DATA LAKE                          │   │
  │  │                 "vieweratlas-data-lake"                    │   │
  │  │                                                            │   │
  │  │  raw/snapshots/         ← live chat JSON     (30-day TTL) │   │
  │  │  raw/vod_chat/          ← VOD chat downloads  (7-day TTL) │   │
  │  │  curated/presence_snapshots/  ← Parquet       (90d→Glacier)│   │
  │  │  curated/analysis/      ← graph + partition results        │   │
  │  └──────────────────────────┬──────────────────────────────────┘   │
  │                             │                                     │
  │            ┌────────────────┼──────────────────┐                  │
  │            ▼                ▼                   ▼                  │
  │  ┌──────────────┐  ┌──────────────┐  ┌──────────────────────┐    │
  │  │ Athena       │  │ CloudWatch   │  │ SNS Alerts           │    │
  │  │ (ad-hoc SQL) │  │ Logs + Dash  │  │ Budget + Task alarms │    │
  │  └──────────────┘  │ + Metrics    │  └──────────────────────┘    │
  │                    └──────────────┘                               │
  └─────────────────────────────────────────────────────────────────────┘

  Estimated monthly cost (100 channels, 4h/day collection, Spot pricing):
    S3 (10 GB):                   ~$0.23
    ECS Fargate (Spot, 4h/day):   ~$3–5
    CloudWatch Logs (1 GB):       ~$0.50
    Secrets Manager (3 secrets):  ~$1.20
    Data Transfer:                ~$0.50
    ─────────────────────────────────────
    TOTAL:                        ~$5–8 / month

  Cost guardrails already in infrastructure/:
    • AWS Budget alert at $50/month (safe-deploy.sh)
    • S3 lifecycle: raw 30d, VOD raw 7d, curated→Glacier 90d
    • CloudWatch log retention: 7 days
    • Task-level: max_runtime_hours, max_collection_cycles, max_vods_per_run


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
APPENDIX — SUGGESTED PRIORITY ORDER
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

  Status     Item  Description
  ─────────  ────  ───────────────────────────────────────────────
  ✅ DONE     —    Fix syntax errors (get_viewers.py, main.py)
  ✅ DONE     —    Remove duplicate code in main.py
  ✅ DONE     —    Fix config YAML loader bugs
  ✅ DONE     —    Update the schema document
  ✅ DONE     —    Add pytest test suite (53 tests)
  ─────────  ────  ───────────────────────────────────────────────
  ✅ DONE     6    Normalize metadata keys across pipeline
  ✅ DONE     7    Validate & harden Docker images
  ✅ DONE     8    Parameterize AWS infrastructure templates
  ✅ DONE     9    Create DEPLOYMENT.md step-by-step guide
  SOON       10    Env var overrides for all config fields
  SOON       11    Wire up CloudWatch custom metrics
  NEXT       12    Daily de-dup + last-24h VOD check
  NEXT       13    Repeat-viewer edge weighting
  NEXT       14    Data retention / cleanup in app layer
  NEXT       15    Bot detection and filtering
  LATER      16    CI/CD pipeline + automated AWS deploy
  LATER      17    Web-based visualization dashboard
  LATER      18    Temporal community tracking
  LATER      19    Leiden algorithm option
  LATER      20    Athena integration for ad-hoc analysis
  FUTURE     21+   Multi-platform, cross-platform, overlapping, scaling

================================================================================
